"""
æµ‹è¯• Stage 2 å¯¹é½æ•ˆæœçš„ä¸“ç”¨è„šæœ¬

Stage 2 å¯¹é½æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡ï¼š
1. å¯¹é½æŸå¤±ï¼ˆAlignment Lossï¼‰
2. ä¸‹ä¸€ä¸ª token é¢„æµ‹å‡†ç¡®ç‡
3. å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
4. åµŒå…¥ç©ºé—´è´¨é‡

ä¸åº”è¯¥è¯„ä¼°ï¼š
- ç”Ÿæˆæ–‡æœ¬è´¨é‡ï¼ˆè¿™æ˜¯ Stage 3 çš„ä»»åŠ¡ï¼‰
"""

import sys
from pathlib import Path

# è·¯å¾„è®¾ç½®
project_root = Path(__file__).resolve().parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import argparse
import torch
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
from src.crome_ts.model import CROMEConfig, StatBypassCROMETS1, get_llm_embed_dim
from src.crome_ts.data_instruct import ChatTSDataset, chatts_collate_fn


def compute_alignment_loss(model, dataloader, device):
    """
    è®¡ç®—å¯¹é½æŸå¤± - Stage 2 çš„æ ¸å¿ƒæŒ‡æ ‡
    è¿™è¡¡é‡æ¨¡å‹èƒ½å¦æ­£ç¡®é¢„æµ‹ä¸‹ä¸€ä¸ª token
    """
    model.eval()
    total_loss = 0
    total_tokens = 0
    correct_predictions = 0
    
    loss_fct = torch.nn.CrossEntropyLoss(reduction='sum')
    
    print("\n" + "="*60)
    print("è®¡ç®—å¯¹é½æŸå¤±ï¼ˆAlignment Lossï¼‰")
    print("="*60)
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Computing Loss"):
            input_texts = batch["input_texts"]
            timeseries_lists = batch["timeseries_lists"]
            output_texts = batch["output_texts"]
            
            try:
                # ä½¿ç”¨ forward_chatts æ–¹æ³•
                model_out = model.forward_chatts(
                    input_texts=input_texts,
                    timeseries_lists=timeseries_lists,
                    output_texts=output_texts,
                    llm_kwargs={}
                )
                
                llm_out = model_out["llm_outputs"]
                logits = llm_out.logits  # [B, Total_L, Vocab]
                
                # è·å–è¾“å‡ºæ–‡æœ¬çš„æ ‡ç­¾
                suffix_labels = model.tokenizer.tokenizer(
                    output_texts, return_tensors="pt", padding=True
                ).input_ids.to(device)
                
                batch_size = logits.shape[0]
                suffix_mask_lengths = model_out["suffix_mask_lengths"]
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ loss å’Œå‡†ç¡®ç‡
                for i in range(batch_size):
                    valid_len = model_out["attention_mask"][i].sum().item()
                    suffix_len = suffix_mask_lengths[i]
                    
                    if suffix_len == 0:
                        continue
                    
                    suffix_start = int(valid_len - suffix_len)
                    
                    # æå– logits å’Œ labels
                    sample_logits = logits[i, suffix_start:suffix_start+suffix_len, :]
                    sample_labels = suffix_labels[i, :suffix_len]
                    
                    if sample_logits.shape[0] > 1:
                        # Causal LM: é¢„æµ‹ä¸‹ä¸€ä¸ª token
                        shift_logits = sample_logits[:-1, :]
                        shift_labels = sample_labels[1:]
                        
                        # è®¡ç®—æŸå¤±
                        loss = loss_fct(shift_logits, shift_labels)
                        total_loss += loss.item()
                        total_tokens += shift_labels.numel()
                        
                        # è®¡ç®—å‡†ç¡®ç‡
                        predictions = shift_logits.argmax(dim=-1)
                        correct = (predictions == shift_labels).sum().item()
                        correct_predictions += correct
                        
            except Exception as e:
                print(f"[Warning] Error processing batch: {e}")
                continue
    
    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
    perplexity = np.exp(avg_loss)
    accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0
    
    return {
        "loss": avg_loss,
        "perplexity": perplexity,
        "accuracy": accuracy,
        "total_tokens": total_tokens
    }


def compute_embedding_quality(model, dataloader, device, num_samples=100):
    """
    è¯„ä¼°åµŒå…¥è´¨é‡
    - åˆ†åˆ«è¯„ä¼° stat_token, query_tokens, detail_tokens çš„èŒƒæ•°
    - åµŒå…¥çš„èŒƒæ•°åˆ†å¸ƒ
    """
    model.eval()
    
    print("\n" + "="*60)
    print("è¯„ä¼°åµŒå…¥ç©ºé—´è´¨é‡ï¼ˆåˆ†åˆ«æ˜¾ç¤º Query å’Œ Detailï¼‰")
    print("="*60)
    
    stat_norms = []
    query_norms = []
    detail_norms = []
    combined_norms = []
    
    with torch.no_grad():
        sample_count = 0
        for batch in tqdm(dataloader, desc="Analyzing Embeddings"):
            if sample_count >= num_samples:
                break
                
            input_texts = batch["input_texts"]
            timeseries_lists = batch["timeseries_lists"]
            output_texts = batch["output_texts"]
            
            for i in range(len(input_texts)):
                if sample_count >= num_samples:
                    break
                    
                ts_list = timeseries_lists[i]
                
                # å¤„ç†æ¯ä¸ªæ—¶é—´åºåˆ—
                for ts_tensor in ts_list[:3]:  # åªçœ‹å‰3ä¸ª
                    ts_tensor = ts_tensor.to(device)
                    ts_batch = ts_tensor.unsqueeze(0)  # [1, T, C]
                    
                    try:
                        # â­ åˆ†åˆ«æå– query å’Œ detail tokens
                        x, stats = model.ts_model.preprocessor(ts_batch)
                        
                        # ä½¿ç”¨ encoder
                        patch_tokens = model.ts_model.shape_encoder(x)
                        
                        # åˆ†åˆ«è·å– query å’Œ detail
                        query_tokens = model.ts_model.qformer(patch_tokens, instruction_embeds=None)
                        detail_tokens = model.ts_model.detail_proj(patch_tokens)
                        
                        # é€šè¿‡ adapter å¤„ç†
                        query_out = model.ts_model.adapter.query_adapter(query_tokens)
                        detail_out = model.ts_model.adapter.patch_adapter(detail_tokens)
                        
                        # æŠ•å½±åˆ° LLM ç©ºé—´
                        query_projected = model.ts_model.llm_proj(query_out)
                        detail_projected = model.ts_model.llm_proj(detail_out)
                        combined_projected = torch.cat([query_projected, detail_projected], dim=1)
                        
                        # è·å– stat token
                        stat_token = model.ts_model.stat_projector(stats)
                        
                        # è®¡ç®—èŒƒæ•°
                        stat_norm = torch.norm(stat_token, dim=-1).mean().item()
                        query_norm = torch.norm(query_projected, dim=-1).mean().item()
                        detail_norm = torch.norm(detail_projected, dim=-1).mean().item()
                        combined_norm = torch.norm(combined_projected, dim=-1).mean().item()
                        
                        stat_norms.append(stat_norm)
                        query_norms.append(query_norm)
                        detail_norms.append(detail_norm)
                        combined_norms.append(combined_norm)
                        
                    except Exception as e:
                        print(f"[Warning] Error processing TS: {e}")
                        continue
                
                sample_count += 1
    
    return {
        "stat_norm_mean": np.mean(stat_norms),
        "stat_norm_std": np.std(stat_norms),
        "query_norm_mean": np.mean(query_norms),
        "query_norm_std": np.std(query_norms),
        "detail_norm_mean": np.mean(detail_norms),
        "detail_norm_std": np.std(detail_norms),
        "combined_norm_mean": np.mean(combined_norms),
        "combined_norm_std": np.std(combined_norms),
    }


def analyze_prediction_patterns(model, dataloader, device, num_samples=20):
    """
    åˆ†æé¢„æµ‹æ¨¡å¼
    - çœ‹æ¨¡å‹åœ¨ä¸åŒä½ç½®çš„ç½®ä¿¡åº¦
    - çœ‹é¢„æµ‹çš„ token åˆ†å¸ƒ
    """
    model.eval()
    
    print("\n" + "="*60)
    print("åˆ†æé¢„æµ‹æ¨¡å¼")
    print("="*60)
    
    top1_confidences = []
    entropy_values = []
    
    with torch.no_grad():
        sample_count = 0
        for batch in tqdm(dataloader, desc="Analyzing Predictions"):
            if sample_count >= num_samples:
                break
                
            input_texts = batch["input_texts"]
            timeseries_lists = batch["timeseries_lists"]
            output_texts = batch["output_texts"]
            
            try:
                model_out = model.forward_chatts(
                    input_texts=input_texts,
                    timeseries_lists=timeseries_lists,
                    output_texts=output_texts,
                    llm_kwargs={}
                )
                
                llm_out = model_out["llm_outputs"]
                logits = llm_out.logits
                
                # è®¡ç®— softmax æ¦‚ç‡
                probs = torch.softmax(logits, dim=-1)
                
                # Top-1 ç½®ä¿¡åº¦
                top1_conf = probs.max(dim=-1).values.mean().item()
                top1_confidences.append(top1_conf)
                
                # ç†µï¼ˆè¡¡é‡ä¸ç¡®å®šæ€§ï¼‰
                entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1).mean().item()
                entropy_values.append(entropy)
                
                sample_count += len(input_texts)
                
            except Exception as e:
                print(f"[Warning] Error: {e}")
                continue
    
    return {
        "avg_top1_confidence": np.mean(top1_confidences),
        "avg_entropy": np.mean(entropy_values),
    }


def main():
    parser = argparse.ArgumentParser(description="Test Stage 2 Alignment Quality")
    parser.add_argument("--jsonl-path", type=str, required=True)
    parser.add_argument("--checkpoint", type=str, required=True)
    parser.add_argument("--seq-len", type=int, default=256)
    parser.add_argument("--patch-len", type=int, default=16)
    parser.add_argument("--patch-stride", type=int, default=8)
    parser.add_argument("--llm-model-path", type=str, default="/root/emhua/btwu/Llama-3.2-3B")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--gpu-id", type=int, default=0)
    parser.add_argument("--num-samples", type=int, default=100, help="ç”¨äºè¯„ä¼°çš„æ ·æœ¬æ•°")
    
    args = parser.parse_args()
    
    # è®¾å¤‡è®¾ç½®
    device_str = args.device
    if "cuda" in args.device and args.gpu_id is not None:
        device_str = f"cuda:{args.gpu_id}"
    device = torch.device(device_str)
    
    print("\n" + "="*70)
    print(" "*20 + "Stage 2 å¯¹é½æ•ˆæœè¯„ä¼°")
    print("="*70)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Data: {args.jsonl_path}")
    print(f"Device: {device}")
    print("="*70)
    
    # ChatTS æ ¼å¼é…ç½®
    input_channels = 1
    
    # åŠ è½½é…ç½®
    llm_embed_dim = get_llm_embed_dim(args.llm_model_path)
    
    config = CROMEConfig(
        input_channels=input_channels,
        llm_embed_dim=llm_embed_dim,
        patch_len=args.patch_len,
        patch_stride=args.patch_stride,
        llm_model_path=args.llm_model_path,
        llm_device_map=device_str,
        llm_dtype="bfloat16",
        use_stats_projector=True,
        epsilon=1e-4,
    )
    
    # åŠ è½½æ¨¡å‹
    print("\n>>> Loading Model...")
    model = StatBypassCROMETS1(config).to(device)
    
    if Path(args.checkpoint).exists():
        checkpoint = torch.load(args.checkpoint, map_location=device)
        missing, unexpected = model.load_state_dict(checkpoint, strict=False)
        print(f">>> Weights Loaded. Missing: {len(missing)}, Unexpected: {len(unexpected)}")
    else:
        raise FileNotFoundError(f"Checkpoint not found: {args.checkpoint}")
    
    model.eval()
    
    # åŠ è½½æ•°æ®
    print(f"\n>>> Loading Dataset...")
    val_ds = ChatTSDataset(
        args.jsonl_path, 
        seq_len=args.seq_len, # ä½œä¸º max_len
        input_channels=input_channels, 
        split="val",
        patch_stride=args.patch_stride # <--- æ–°å¢ï¼šä¼ å…¥ stride ä»¥å¯ç”¨åŠ¨æ€å¯¹é½å’Œ Edge Padding
    )
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if args.num_samples > 0 and args.num_samples < len(val_ds):
        val_ds.records = val_ds.records[:args.num_samples]
    
    val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=chatts_collate_fn)
    
    print(f">>> Evaluating on {len(val_ds)} samples...")
    
    # ==================== æ ¸å¿ƒè¯„ä¼° ====================
    
    # 1. å¯¹é½æŸå¤±ï¼ˆæœ€é‡è¦ï¼‰
    loss_metrics = compute_alignment_loss(model, val_loader, device)
    
    # 2. åµŒå…¥è´¨é‡
    embedding_metrics = compute_embedding_quality(model, val_loader, device, num_samples=50)
    
    # 3. é¢„æµ‹æ¨¡å¼
    prediction_metrics = analyze_prediction_patterns(model, val_loader, device, num_samples=20)
    
    # ==================== è¾“å‡ºç»“æœ ====================
    
    print("\n" + "="*70)
    print(" "*25 + "è¯„ä¼°ç»“æœ")
    print("="*70)
    
    print("\nğŸ“Š 1. å¯¹é½æŸå¤±ï¼ˆAlignment Lossï¼‰")
    print("-" * 70)
    print(f"  Loss (per token):        {loss_metrics['loss']:.4f}")
    print(f"  Perplexity:              {loss_metrics['perplexity']:.4f}")
    print(f"  Next Token Accuracy:     {loss_metrics['accuracy']*100:.2f}%")
    print(f"  Total Tokens Evaluated:  {loss_metrics['total_tokens']}")
    
    # è§£è¯»
    print("\n  ğŸ’¡ è§£è¯»:")
    if loss_metrics['loss'] < 0.5:
        print("     âœ… ä¼˜ç§€ï¼å¯¹é½æ•ˆæœå¾ˆå¥½")
    elif loss_metrics['loss'] < 0.8:
        print("     âš ï¸  ä¸€èˆ¬ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
    elif loss_metrics['loss'] < 1.2:
        print("     âŒ è¾ƒå·®ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒ")
    else:
        print("     âŒ å¾ˆå·®ï¼Œæ£€æŸ¥è®­ç»ƒè¿‡ç¨‹")
    
    print("\nğŸ“ 2. åµŒå…¥ç©ºé—´è´¨é‡ï¼ˆåˆ†åˆ«æ˜¾ç¤º Query å’Œ Detailï¼‰")
    print("-" * 70)
    print(f"  Stat Token Norm:         {embedding_metrics['stat_norm_mean']:.4f} Â± {embedding_metrics['stat_norm_std']:.4f}")
    print(f"  Query Token Norm:        {embedding_metrics['query_norm_mean']:.4f} Â± {embedding_metrics['query_norm_std']:.4f}  (å…³é”®ä¿¡æ¯)")
    print(f"  Detail Token Norm:       {embedding_metrics['detail_norm_mean']:.4f} Â± {embedding_metrics['detail_norm_std']:.4f}  (ç»†èŠ‚ä¿¡æ¯)")
    print(f"  Combined (Q+D) Norm:     {embedding_metrics['combined_norm_mean']:.4f} Â± {embedding_metrics['combined_norm_std']:.4f}  (æ‹¼æ¥å)")
    
    print("\n  ğŸ’¡ è§£è¯»:")
    stat_norm = embedding_metrics['stat_norm_mean']
    query_norm = embedding_metrics['query_norm_mean']
    detail_norm = embedding_metrics['detail_norm_mean']
    combined_norm = embedding_metrics['combined_norm_mean']
    
    issues = []
    if stat_norm < 10 or stat_norm > 200:
        issues.append("Stat token èŒƒæ•°å¼‚å¸¸")
    if query_norm < 10 or query_norm > 200:
        issues.append("Query token èŒƒæ•°å¼‚å¸¸")
    if detail_norm < 10 or detail_norm > 200:
        issues.append("Detail token èŒƒæ•°å¼‚å¸¸")
    
    if not issues:
        print("     âœ… æ‰€æœ‰åµŒå…¥èŒƒæ•°æ­£å¸¸ï¼Œåœ¨åˆç†èŒƒå›´å†…")
    else:
        print(f"     âš ï¸  {', '.join(issues)}")
    
    # æ£€æŸ¥ç¨³å®šæ€§
    query_std = embedding_metrics['query_norm_std']
    detail_std = embedding_metrics['detail_norm_std']
    if query_std < 5 and detail_std < 5:
        print("     âœ… Query å’Œ Detail éƒ½éå¸¸ç¨³å®šï¼ˆæ ‡å‡†å·®å°ï¼‰")
    elif query_std < 10 and detail_std < 10:
        print("     âœ… Query å’Œ Detail ç¨³å®šæ€§è‰¯å¥½")
    else:
        print("     âš ï¸  åµŒå…¥å˜åŒ–è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ")
    
    print("\nğŸ¯ 3. é¢„æµ‹æ¨¡å¼")
    print("-" * 70)
    print(f"  Avg Top-1 Confidence:    {prediction_metrics['avg_top1_confidence']:.4f}")
    print(f"  Avg Entropy:             {prediction_metrics['avg_entropy']:.4f}")
    
    print("\n  ğŸ’¡ è§£è¯»:")
    conf = prediction_metrics['avg_top1_confidence']
    if conf > 0.5:
        print("     âœ… æ¨¡å‹é¢„æµ‹è¾ƒè‡ªä¿¡")
    elif conf > 0.3:
        print("     âš ï¸  æ¨¡å‹é¢„æµ‹ä¸­ç­‰è‡ªä¿¡")
    else:
        print("     âŒ æ¨¡å‹é¢„æµ‹ä¸è‡ªä¿¡ï¼Œå¯èƒ½æœªå­¦å¥½")
    
    # ==================== æ€»ç»“ä¸å»ºè®® ====================
    
    print("\n" + "="*70)
    print(" "*25 + "æ€»ç»“ä¸å»ºè®®")
    print("="*70)
    
    loss = loss_metrics['loss']
    acc = loss_metrics['accuracy']
    
    print("\nğŸ“ å½“å‰æ¨¡å‹çŠ¶æ€:")
    if loss < 0.5 and acc > 0.5:
        print("   âœ… å¯¹é½æ•ˆæœä¼˜ç§€ï¼Œå¯ä»¥è¿›å…¥ Stage 3 è®­ç»ƒ")
    elif loss < 0.8 and acc > 0.3:
        print("   âš ï¸  å¯¹é½æ•ˆæœä¸€èˆ¬ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒ Stage 2")
        print("      ç›®æ ‡: Loss < 0.5, Accuracy > 50%")
    else:
        print("   âŒ å¯¹é½æ•ˆæœè¾ƒå·®ï¼Œéœ€è¦ç»§ç»­è®­ç»ƒ")
        print("      å»ºè®®: æ£€æŸ¥å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ã€æ•°æ®è´¨é‡")
    
    print("\nğŸ“ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
    if loss < 0.5:
        print("   1. âœ… å½“å‰ Stage 2 å·²è®­ç»ƒå¥½")
        print("   2. ğŸ¯ å¯ä»¥å¼€å§‹è®­ç»ƒ Stage 3")
        print("   3. ğŸ“Š ä½¿ç”¨ test_chatts_instruct.py è¯„ä¼° Stage 3")
    else:
        print("   1. ğŸ”„ ç»§ç»­è®­ç»ƒ Stage 2")
        print(f"      å½“å‰: Loss = {loss:.4f}")
        print(f"      ç›®æ ‡: Loss < 0.5")
        print("   2. âš™ï¸  å¯ä»¥å°è¯•:")
        print("      - å¢åŠ è®­ç»ƒè½®æ•° (--epochs 20)")
        print("      - è°ƒæ•´å­¦ä¹ ç‡ (--lr 2e-4)")
        print("      - ä½¿ç”¨æ›´å¤šæ•°æ®")
    
    print("\nâš ï¸  é‡è¦æé†’:")
    print("   - Stage 2 çš„ç›®æ ‡æ˜¯å¯¹é½ï¼Œä¸æ˜¯ç”Ÿæˆ")
    print("   - ä¸è¦ç”¨ç”Ÿæˆè´¨é‡è¯„ä¼° Stage 2")
    print("   - åªæœ‰ Stage 3 æ‰èƒ½ç”Ÿæˆå¥½çš„æ–‡æœ¬")
    
    print("\n" + "="*70)
    print(" "*20 + "è¯„ä¼°å®Œæˆ")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()

