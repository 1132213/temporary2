import sys
import os
from pathlib import Path

# ç¦ç”¨ tokenizers å¹¶è¡Œä»¥é¿å…å¤šè¿›ç¨‹è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# è·¯å¾„è®¾ç½®ï¼šç¡®ä¿èƒ½æ‰¾åˆ° src
project_root = Path(__file__).resolve().parent.parent
project_root_str = str(project_root)
if project_root_str not in sys.path:
    sys.path.insert(0, project_root_str)

import argparse
import torch
from torch.utils.data import DataLoader
from src.crome_ts.model import CROMEConfig, StatBypassCROMETS1, get_llm_embed_dim
from src.crome_ts.data_instruct import ChatTSDataset, chatts_collate_fn

def build_chatts_embeddings_for_inference(model, input_text, ts_list, device):
    """
    ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ–¹å¼æ„å»º ChatTS æ ¼å¼çš„åµŒå…¥ï¼ˆç”¨äºæ¨ç†ï¼‰
    è¿™ä¸ªå‡½æ•°å¤ç”¨äº†æ¨¡å‹çš„ forward_chatts ä¸­çš„é€»è¾‘ï¼Œä½†ä¸åŒ…å« output_text éƒ¨åˆ†
    
    å‚æ•°ï¼š
        model: æ¨¡å‹å®ä¾‹
        input_text: åŒ…å« <ts><ts/> æ ‡è®°çš„è¾“å…¥æ–‡æœ¬
        ts_list: æ—¶é—´åºåˆ—åˆ—è¡¨ [tensor1, tensor2, ...]
        device: è®¾å¤‡
    
    è¿”å›ï¼š
        assembled_embeds: ç»„è£…å¥½çš„åµŒå…¥ [1, total_length, llm_embed_dim]
        attention_mask: æ³¨æ„åŠ›æ©ç  [1, total_length]
    """
    # åˆ†å‰²è¾“å…¥æ–‡æœ¬ï¼šæ‰¾åˆ°æ‰€æœ‰ <ts><ts/> æ ‡è®°å¹¶åˆ†å‰²
    ts_marker = "<ts><ts/>"
    text_parts = input_text.split(ts_marker)
    
    # ç¡®ä¿æ—¶é—´åºåˆ—æ•°é‡ä¸æ ‡è®°æ•°é‡åŒ¹é…
    num_markers = len(text_parts) - 1
    num_timeseries = len(ts_list)
    
    timeseries_list = list(ts_list)  # å¤åˆ¶ä»¥é¿å…ä¿®æ”¹åŸåˆ—è¡¨
    
    if num_timeseries < num_markers:
        # å¦‚æœæ—¶é—´åºåˆ—ä¸å¤Ÿï¼Œç”¨é›¶å¡«å……
        for _ in range(num_markers - num_timeseries):
            timeseries_list.append(
                torch.zeros(model.config.seq_len, model.config.input_channels, device=device)
            )
    elif num_timeseries > num_markers:
        # å¦‚æœæ—¶é—´åºåˆ—å¤ªå¤šï¼Œåªä½¿ç”¨å‰ num_markers ä¸ª
        timeseries_list = timeseries_list[:num_markers]
    
    # æ”¶é›†æ‰€æœ‰ç‰‡æ®µçš„åµŒå…¥
    segment_embeds = []
    segment_masks = []
    
    # ç¡®å®šç›®æ ‡ dtype
    target_dtype = next(model.llm.parameters()).dtype
    
    # è·å– tokenizer
    tokenizer = model.tokenizer.tokenizer
    
    # å¤„ç†ç¬¬ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µï¼ˆprefixï¼‰
    if text_parts[0]:
        # ä½¿ç”¨ tokenizer ç›´æ¥ç¼–ç ï¼Œä¸é€šè¿‡ model.tokenizer() ä»¥é¿å…å¤šæ¬¡æ·»åŠ ç‰¹æ®Štoken
        prefix_tokens = tokenizer(
            text_parts[0], 
            return_tensors="pt", 
            add_special_tokens=True  # åªåœ¨ç¬¬ä¸€ä¸ªç‰‡æ®µæ·»åŠ  BOS
        ).to(device)
        prefix_embed = model.llm.embed(prefix_tokens.input_ids)  # [1, L, D]
        prefix_mask = prefix_tokens.attention_mask  # [1, L]
        segment_embeds.append(prefix_embed[0])  # [L, D]
        segment_masks.append(prefix_mask[0])  # [L]
    
    # å¤„ç†æ¯ä¸ªæ—¶é—´åºåˆ—å’Œåç»­æ–‡æœ¬ç‰‡æ®µ
    for ts_idx, ts_tensor in enumerate(timeseries_list):
        # ç¡®ä¿æ—¶é—´åºåˆ—åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        ts_tensor = ts_tensor.to(device)
        
        # æ·»åŠ  batch ç»´åº¦ï¼š[T, C] -> [1, T, C]
        ts_batch = ts_tensor.unsqueeze(0)
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—çš„åµŒå…¥ï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ–¹æ³•ï¼‰
        stat_token, ts_tokens = model.ts_model._process_single_channel(
            ts_batch, instruction_embeds=None
        )
        # stat_token: [1, 1, D], ts_tokens: [1, N, D]
        
        # ç¡®ä¿æ•°æ®ç±»å‹å¯¹é½
        if stat_token.dtype != target_dtype:
            stat_token = stat_token.to(dtype=target_dtype)
        if ts_tokens.dtype != target_dtype:
            ts_tokens = ts_tokens.to(dtype=target_dtype)
        
        # ç»„è£…ï¼š[Stat][TS_Tokens]
        ts_embed = torch.cat([stat_token[0], ts_tokens[0]], dim=0)  # [1+N, D]
        
        # æ·»åŠ åˆ°ç‰‡æ®µåˆ—è¡¨
        segment_embeds.append(ts_embed)
        segment_masks.append(torch.ones(ts_embed.shape[0], device=device, dtype=torch.long))
        
        # æ·»åŠ  SEP tokenï¼ˆå¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ—¶é—´åºåˆ—ï¼‰
        if ts_idx < len(timeseries_list) - 1:
            sep_embed = model.sep_token  # [1, D]
            # ç¡®ä¿ dtype å¯¹é½
            if sep_embed.dtype != target_dtype:
                sep_embed = sep_embed.to(dtype=target_dtype)
            segment_embeds.append(sep_embed)  # [1, D]
            segment_masks.append(torch.ones(1, device=device, dtype=torch.long))
        
        # å¤„ç†è¯¥æ—¶é—´åºåˆ—åçš„æ–‡æœ¬ç‰‡æ®µ
        text_idx = ts_idx + 1
        if text_idx < len(text_parts) and text_parts[text_idx]:
            # ä¸­é—´æ–‡æœ¬ç‰‡æ®µä¸æ·»åŠ ç‰¹æ®Š token
            text_tokens = tokenizer(
                text_parts[text_idx], 
                return_tensors="pt", 
                add_special_tokens=False  # ä¸­é—´ç‰‡æ®µä¸æ·»åŠ ç‰¹æ®Štoken
            ).to(device)
            text_embed = model.llm.embed(text_tokens.input_ids)  # [1, L, D]
            text_mask = text_tokens.attention_mask  # [1, L]
            segment_embeds.append(text_embed[0])  # [L, D]
            segment_masks.append(text_mask[0])  # [L]
    
    # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
    if segment_embeds:
        full_embed = torch.cat(segment_embeds, dim=0)  # [Total_L, D]
        full_mask = torch.cat(segment_masks, dim=0)  # [Total_L]
        
        # â­ æ·»åŠ ç”Ÿæˆè§¦å‘ tokenï¼ˆBOSï¼‰åˆ°æœ«å°¾
        # è¿™å‘Šè¯‰ LLM "è¾“å…¥ç»“æŸï¼Œç°åœ¨å¼€å§‹ç”Ÿæˆ"
        bos_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 1
        bos_tensor = torch.tensor([[bos_token_id]], device=device)
        bos_embed = model.llm.embed(bos_tensor)  # [1, 1, D]
        bos_mask = torch.ones(1, device=device, dtype=torch.long)
        
        # æ‹¼æ¥ BOS token
        full_embed = torch.cat([full_embed, bos_embed[0]], dim=0)  # [Total_L+1, D]
        full_mask = torch.cat([full_mask, bos_mask], dim=0)  # [Total_L+1]
        
        # æ·»åŠ  batch ç»´åº¦
        assembled_embeds = full_embed.unsqueeze(0)  # [1, Total_L+1, D]
        attention_mask = full_mask.unsqueeze(0)  # [1, Total_L+1]
    else:
        # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œè¿”å›ç©ºåµŒå…¥
        assembled_embeds = torch.empty(1, 0, model.config.llm_embed_dim, device=device)
        attention_mask = torch.empty(1, 0, device=device, dtype=torch.long)
    
    return assembled_embeds, attention_mask

def generate_response(model, ts_list, input_text, device, max_new_tokens=128):
    """
    æ‰§è¡Œæ¨ç†ç”Ÿæˆï¼ˆé’ˆå¯¹ ChatTS æ ¼å¼ï¼Œä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„åµŒå…¥æ„å»ºæ–¹å¼ï¼‰
    
    å‚æ•°ï¼š
        model: æ¨¡å‹å®ä¾‹
        ts_list: æ—¶é—´åºåˆ—åˆ—è¡¨
        input_text: åŒ…å« <ts><ts/> æ ‡è®°çš„è¾“å…¥æ–‡æœ¬ï¼ˆä¸å« User:/Assistant: å‰ç¼€ï¼‰
        device: è®¾å¤‡
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
    """
    tokenizer = model.tokenizer.tokenizer
    
    # ç¡®ä¿ Pad Token ID æœ‰æ•ˆ
    if tokenizer.pad_token_id is None or tokenizer.pad_token_id == tokenizer.eos_token_id:
        tokenizer.pad_token_id = tokenizer.unk_token_id if tokenizer.unk_token_id is not None else 0
    
    # æ„å»ºå®Œæ•´çš„è¾“å…¥æ–‡æœ¬ï¼ˆåŒ…å« User: å’Œ Assistant: æç¤ºï¼‰
    full_input = f"User: {input_text}\nAssistant: "
    
    with torch.no_grad():
        # ä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ–¹å¼æ„å»ºåµŒå…¥
        inputs_embeds, attention_mask = build_chatts_embeddings_for_inference(
            model, full_input, ts_list, device
        )
        
        # æ£€æŸ¥è¾“å…¥é•¿åº¦
        input_length = inputs_embeds.shape[1]
        print(f"[Debug] Input embeddings length: {input_length}")
        
        # å¦‚æœè¾“å…¥å¤ªé•¿ï¼Œå¯èƒ½éœ€è¦è­¦å‘Š
        if input_length > 4000:
            print(f"[Warning] Input length ({input_length}) is very long, generation may be unstable")
        
        # ä¸º Stage 2 å¯¹é½æ¨¡å‹ä½¿ç”¨æ›´ä¿å®ˆçš„ç”Ÿæˆå‚æ•°
        output_ids = model.llm.model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            min_new_tokens=10,  # è‡³å°‘ç”Ÿæˆ10ä¸ªtoken
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            bos_token_id=tokenizer.bos_token_id,
            # Stage 2 ä½¿ç”¨æ›´ä¿å®ˆçš„é‡‡æ ·ç­–ç•¥
            do_sample=True,
            temperature=0.8,
            top_p=0.95,
            top_k=50,
            repetition_penalty=1.2,
            length_penalty=1.0,
            no_repeat_ngram_size=3,  # é¿å…é‡å¤3-gram
        )
        
    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    # æ‰“å°ç”Ÿæˆçš„ token IDs ç”¨äºè°ƒè¯•
    print(f"[Debug] Generated {len(output_ids[0])} tokens")
    print(f"[Debug] First 20 token IDs: {output_ids[0][:20].tolist()}")
    
    return generated_text

def main(args):
    # è®¾å¤‡å¤„ç†
    device_str = args.device
    if args.device.startswith("cuda") and args.gpu_id is not None:
        device_str = f"cuda:{args.gpu_id}"
    device = torch.device(device_str)
    
    print(f">>> Loading Config & Model...")
    
    # 1. ChatTS æ ¼å¼ä¸­æ¯ä¸ªæ—¶é—´åºåˆ—éƒ½æ˜¯å•é€šé“ï¼ˆä¸è®­ç»ƒä»£ç ä¿æŒä¸€è‡´ï¼‰
    # å¤šä¸ªæ—¶é—´åºåˆ—é€šè¿‡ forward_chatts() æ–¹æ³•å¤„ç†ï¼Œè€Œä¸æ˜¯å¤šé€šé“æ¨¡å¼
    input_channels = 1
    print(f">>> ChatTS format uses input_channels = {input_channels} (each TS is single-channel)")
    
    # 2. é…ç½®
    llm_embed_dim = get_llm_embed_dim(args.llm_model_path)
    print(f">>> LLM Embedding Dimension: {llm_embed_dim} (from {args.llm_model_path})")
    
    config = CROMEConfig(
        input_channels=input_channels,
        llm_embed_dim=llm_embed_dim,
        patch_len=args.patch_len,
        patch_stride=args.patch_stride,
        llm_model_path=args.llm_model_path,
        llm_device_map=device_str,
        llm_dtype="bfloat16",
        use_stats_projector=True,
        epsilon=1e-5,
    )
    
    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = StatBypassCROMETS1(config).to(device)
    
    # 4. åŠ è½½ Stage 2 è®­ç»ƒå¥½çš„å¯¹é½æƒé‡
    print(f">>> Loading Stage 2 Checkpoint from {args.checkpoint}...")
    if Path(args.checkpoint).exists():
        checkpoint = torch.load(args.checkpoint, map_location=device)
        
        # å…³é”®ï¼šstrict=False
        missing, unexpected = model.load_state_dict(checkpoint, strict=False)
        
        # éªŒè¯æ˜¯å¦åŠ è½½æˆåŠŸ
        real_missing = [k for k in missing if "llm.model" not in k]
        if len(real_missing) > 0:
            print(f"!!! Warning: Potential missing keys: {real_missing[:5]} ...")
        else:
            print(">>> Stage 2 Weights merged successfully (LLM weights skipped as expected).")
    else:
        print(f"!!! Error: Checkpoint {args.checkpoint} not found!")
        return

    model.eval()
    
    # 4. åŠ è½½éªŒè¯æ•°æ®ï¼ˆChatTS æ ¼å¼ï¼‰
    print(f">>> Loading ChatTS Validation Dataset...")
    val_ds = ChatTSDataset(args.jsonl_path, args.seq_len, input_channels, split="val")
    val_loader = DataLoader(val_ds, batch_size=1, shuffle=True, collate_fn=chatts_collate_fn)
    
    print(f"\n{'='*60}")
    print(f"Starting Stage 2 Alignment Test (ChatTS Format) on {args.num_samples} samples")
    print(f"{'='*60}")
    print(f"\nâš ï¸  IMPORTANT NOTES:")
    print(f"1. Stage 2 (Alignment) models may not generate coherent text yet")
    print(f"2. They are trained to align TS embeddings with LLM space")
    print(f"3. For better text generation, use Stage 3 (Instruct) models")
    print(f"4. Long inputs (many TS) may cause generation issues\n")
    print(f"{'='*60}\n")
    
    sample_count = 0
    for batch in val_loader:
        if sample_count >= args.num_samples:
            break
        
        input_texts = batch["input_texts"]
        timeseries_lists = batch["timeseries_lists"]
        output_texts = batch["output_texts"]
        
        # å¤„ç†ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆbatch_size=1ï¼‰
        input_text = input_texts[0]
        ts_list = timeseries_lists[0]
        ground_truth = output_texts[0]
        
        try:
            # è®¡ç®—è¾“å…¥ç»Ÿè®¡
            input_text_len = len(input_text)
            num_ts = len(ts_list)
            ts_shapes = [ts.shape for ts in ts_list]
            
            # ç”Ÿæˆï¼ˆä½¿ç”¨äº¤æ›¿çš„æ–‡æœ¬å’Œæ—¶é—´åºåˆ—æ ¼å¼ï¼‰
            prediction = generate_response(model, ts_list, input_text, device)
            
            print(f"\n{'='*60}")
            print(f"Sample #{sample_count+1}")
            print(f"{'='*60}")
            print(f"ğŸ“Š Input Statistics:")
            print(f"  - Text length: {input_text_len} chars")
            print(f"  - Number of time series: {num_ts}")
            print(f"  - Time series shapes: {ts_shapes}")
            print(f"\n--- Input Text (with <ts><ts/> markers) ---")
            # åªæ˜¾ç¤ºå‰500ä¸ªå­—ç¬¦ï¼Œé¿å…è¿‡é•¿
            display_text = input_text.strip()
            if len(display_text) > 500:
                display_text = display_text[:500] + "... (truncated)"
            print(f"{display_text}")
            print(f"\n--- Ground Truth ---")
            print(f"{ground_truth.strip()}")
            print(f"\n--- Stage 2 Output ---")
            print(f"{prediction.strip()}")
            print(f"{'-'*60}\n")
            
            sample_count += 1
            
        except Exception as e:
            print(f"[Error] Sample {sample_count}: {e}")
            import traceback
            traceback.print_exc()
            sample_count += 1

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test Stage 2 Alignment with ChatTS Format")
    parser.add_argument("--jsonl-path", type=str, default="chatts_data.jsonl", help="ChatTS æ ¼å¼çš„æ•°æ®æ–‡ä»¶")
    parser.add_argument("--checkpoint", type=str, default="crome_stage2_aligned.pth", help="Stage 2 æƒé‡è·¯å¾„")
    
    parser.add_argument("--seq-len", type=int, default=1024)
    # ç§»é™¤ input-channels å‚æ•°ï¼Œæ”¹ä¸ºè‡ªåŠ¨æ£€æµ‹
    parser.add_argument("--patch-len", type=int, default=32)
    parser.add_argument("--patch-stride", type=int, default=16)
    parser.add_argument("--llm-model-path", type=str, default="/root/emhua/btwu/Llama-2-7b-hf")
    
    parser.add_argument("--num-samples", type=int, default=5, help="è¦æµ‹è¯•çš„æ ·æœ¬æ•°é‡")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--gpu-id", type=int, default=None)
    
    args = parser.parse_args()
    main(args)

