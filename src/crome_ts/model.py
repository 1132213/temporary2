from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Optional, Sequence, Tuple, Union
import math

import torch
from torch import Tensor, nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig


class InstructionRefiner(nn.Module):
    def __init__(self, config: CROMEConfig):
        super().__init__()
        self.embed_dim = config.llm_embed_dim
        self.num_queries = config.num_task_queries
        
        # 1. å¯å­¦ä¹ çš„ä»»åŠ¡æŸ¥è¯¢å‘é‡ (Task Queries)
        # å½¢çŠ¶: [1, Num_Queries, LLM_Dim]
        self.task_queries = nn.Parameter(
            torch.randn(1, self.num_queries, self.embed_dim)
        )
        nn.init.normal_(self.task_queries, std=0.02)
        
        # 2. Cross-Attention å±‚
        # Query = Task Queries
        # Key/Value = Raw Instruction Embeddings
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=self.embed_dim,
            num_heads=4, # 4å¤´è¶³ä»¥å¤„ç†è¯­ä¹‰èšç±»
            batch_first=True,
            dropout=config.proj_dropout
        )
        
        # 3. LayerNorm (ç¨³å®šè®­ç»ƒ)
        self.norm = nn.LayerNorm(self.embed_dim)

    def forward(self, instruction_embeds: Tensor) -> Tensor:
        if instruction_embeds is None:
            return None

        B = instruction_embeds.shape[0]
        ref = instruction_embeds  # â­ dtype / device çš„å”¯ä¸€æƒå¨æ¥æº

        # 1. Queries å¯¹é½
        queries = self.task_queries.expand(B, -1, -1).to(
            dtype=ref.dtype,
            device=ref.device
        )

        # 2. Cross-Attention å‚æ•°å¯¹é½
        if self.cross_attn.in_proj_weight.dtype != ref.dtype:
            self.cross_attn = self.cross_attn.to(
                dtype=ref.dtype,
                device=ref.device
            )

        # 3. LayerNorm å‚æ•°å¯¹é½ï¼ˆğŸ”¥ è¿™æ¬¡æŠ¥é”™çš„æ ¹å› ï¼‰
        if self.norm.weight.dtype != ref.dtype:
            self.norm = self.norm.to(
                dtype=ref.dtype,
                device=ref.device
            )

        # 4. Cross Attention
        task_embeds, _ = self.cross_attn(
            query=queries,
            key=ref,
            value=ref
        )

        # 5. Norm
        return self.norm(task_embeds)




class CNNDetailEncoder(nn.Module):
    def __init__(self, input_channels, patch_embedding_dim, dropout=0.1):
        super().__init__()
        
        # éšè—å±‚ç»´åº¦
        hidden_dim = 64 
        
        # ç¬¬ä¸€å±‚æŠ•å½±
        self.first_conv = nn.Conv1d(input_channels, hidden_dim, kernel_size=1)
        
        # å®šä¹‰ 3 ä¸ªæ ‡å‡†çš„æ®‹å·®å—
        # åšæŒ Kernel=3, Dilation=1 (ä¸è†¨èƒ€), Padding=1 (ä¿æŒé•¿åº¦)
        # è¿™ç§ç»“æ„åªå…³æ³¨ "t" æ—¶åˆ»åŠå…¶å·¦å³é‚»å±…ï¼Œéå¸¸ç¬¦åˆ "Micro" çš„å®šä¹‰
        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm1d(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm1d(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout)
            ) for _ in range(3) # å †å  3 å±‚ï¼Œè¶³ä»¥æå–å¤æ‚çš„å¾®è§‚ç‰¹å¾
        ])
        
        # æœ€ç»ˆæŠ•å½±
        self.final_proj = nn.Conv1d(hidden_dim, patch_embedding_dim, kernel_size=1)

    def forward(self, x):
        # x: [Batch, Channels, Seq_Len]
        x = self.first_conv(x)
        
        for layer in self.layers:
            residual = x
            x = layer(x)
            x = x + residual # ResNet é£æ ¼è¿æ¥ï¼Œè®­ç»ƒæ›´ç¨³å®š
            
        x = self.final_proj(x)
        return x.transpose(1, 2) # [B, L, D]
class FusionGate(nn.Module):
    def __init__(self, config: CROMEConfig):
        super().__init__()
        # [ä¿®æ”¹] è¾“å‡ºç»´åº¦ä» 1 æ”¹ä¸º patch_embedding_dim (ä¾‹å¦‚ 512)
        self.proj = nn.Linear(config.llm_embed_dim, config.patch_embedding_dim)
        
    def forward(self, instruction_embeds):
        if instruction_embeds is None:
            return 0.5 
        
        pooled = instruction_embeds.mean(dim=1)
        pooled = pooled.to(self.proj.weight.dtype)
        
        # [ä¿®æ”¹] è¾“å‡ºå½¢çŠ¶ä¸º [B, 1, D]
        # è¿™æ · Gate å¯ä»¥åœ¨ä¸åŒçš„ç‰¹å¾é€šé“ä¸Šåˆ†åˆ«å†³å®šâ€œå¬å®è§‚çš„â€è¿˜æ˜¯â€œå¬å¾®è§‚çš„â€
        gate = torch.sigmoid(self.proj(pooled)).unsqueeze(1) 
        return gate
# ---2. ç‰¹å¾èåˆé—¨æ§ ---
# class FusionGate(nn.Module):
#     """
#     æ ¹æ®æŒ‡ä»¤è¯­ä¹‰ï¼ŒåŠ¨æ€å†³å®šå…³æ³¨å®è§‚è¶‹åŠ¿è¿˜æ˜¯å¾®è§‚ç»†èŠ‚ã€‚
#     """
#     def __init__(self, config: CROMEConfig):
#         super().__init__()
#         # è¾“å…¥æ˜¯ LLM çš„ Instruction Embedding
#         self.proj = nn.Linear(config.llm_embed_dim, 1)
        
#     def forward(self, instruction_embeds):
#         # instruction_embeds: [B, L_text, D_llm]
#         if instruction_embeds is None:
#             # å¦‚æœæ²¡æœ‰æŒ‡ä»¤ï¼Œé»˜è®¤ 50/50 æ··åˆ
#             return 0.5 
        
#         # å¯¹æ–‡æœ¬åºåˆ—å–å‡å€¼ä½œä¸ºå¥å­è¡¨ç¤º
#         # [B, L, D] -> [B, D]
#         pooled = instruction_embeds.mean(dim=1)
#         pooled = pooled.to(self.proj.weight.dtype)
        
#         # Sigmoid æ˜ å°„åˆ° [0, 1]
#         # output: [B, 1, 1] ç”¨äºå¹¿æ’­
#         gate = torch.sigmoid(self.proj(pooled)).unsqueeze(1)
#         return gate


class RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=4096, base=10000, device=None):
        super().__init__()
        self.dim = dim
        self.base = base
        self.max_position_embeddings = max_position_embeddings
        # é¢„è®¡ç®—é¢‘ç‡
        self._set_cos_sin_cache(seq_len=max_position_embeddings, device=device)

    def _set_cos_sin_cache(self, seq_len, device):
        self.max_seq_len_cached = seq_len
        t = torch.arange(seq_len, device=device, dtype=torch.float32)
        
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.float32, device=device) / self.dim))
        freqs = torch.outer(t, inv_freq)
        # Different from some implementations, we concat in last dim to match (d/2) pair
        emb = torch.cat((freqs, freqs), dim=-1)
        
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :], persistent=False)
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :], persistent=False)

    def forward(self, x, seq_len):
        # x: [Batch, Heads, Seq_Len, Head_Dim]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len=seq_len + 128, device=x.device)
            
        return (
            self.cos_cached[..., :seq_len, :].to(dtype=x.dtype),
            self.sin_cached[..., :seq_len, :].to(dtype=x.dtype),
        )

def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin):
    # q, k: [Batch, Heads, Seq_Len, Head_Dim]
    # cos, sin: [1, 1, Seq_Len, Head_Dim]
    # ç¡®ä¿ cos/sin ä¸ q/k å¹¿æ’­å…¼å®¹
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

# === æ”¯æŒ RoPE çš„è‡ªå®šä¹‰ Transformer Block ===

class RoPESelfAttention(nn.Module):
    def __init__(self, dim, num_heads, dropout=0.1):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3)
        self.proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, rotary_emb):
        B, L, D = x.shape
        # [B, L, 3*D] -> [B, L, 3, Heads, Head_Dim] -> [3, B, Heads, L, Head_Dim]
        qkv = self.qkv(x).reshape(B, L, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Apply RoPE
        cos, sin = rotary_emb(v, seq_len=L)
        q, k = apply_rotary_pos_emb(q, k, cos, sin)

        # Attention: [B, Heads, L, L]
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.dropout(attn)

        # Output: [B, L, D]
        x = (attn @ v).transpose(1, 2).reshape(B, L, D)
        x = self.proj(x)
        x = self.dropout(x)
        return x

class RoPETransformerBlock(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = RoPESelfAttention(dim, num_heads, dropout=dropout)
        
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, int(dim * mlp_ratio)),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(int(dim * mlp_ratio), dim),
            nn.Dropout(dropout)
        )

    def forward(self, x, rotary_emb):
        # Pre-Norm ç»“æ„ï¼Œæ›´ç¨³å®š
        x = x + self.attn(self.norm1(x), rotary_emb)
        x = x + self.mlp(self.norm2(x))
        return x
        
@dataclass
class CROMEConfig:
    """
    å…¨å±€é…ç½®ã€‚
    """
    input_channels: int
    llm_embed_dim: int
    patch_len: int = 16
    patch_stride: int = 8
    patch_embedding_dim: int = 512
    patch_num_heads: int = 8
    patch_num_layers: int = 4
    freeze_patch_encoder: bool = False
    query_tokens: int = 64
    num_task_queries: int = 8
    adapter_hidden_dim: int = 256
    fuse_mode: str = "add"
    epsilon: float = 1e-4
    proj_dropout: float = 0.0
    # LLM æ¥å£
    llm_model_path: str = "/mnt/shared-storage-user/dllm-share/Models/Qwen3/Qwen3-8B"
    llm_dtype: str = "bfloat16"
    llm_device_map: str = "auto"


def _resolve_dtype(name: str) -> torch.dtype:
    name = name.lower()
    if not hasattr(torch, name):
        raise ValueError(f"æ— æ³•è§£æ dtype: {name}")
    return getattr(torch, name)


def get_llm_embed_dim(llm_model_path: str) -> int:
    try:
        config = AutoConfig.from_pretrained(llm_model_path)
        if hasattr(config, 'hidden_size'):
            return config.hidden_size
        elif hasattr(config, 'd_model'):
            return config.d_model
        elif hasattr(config, 'n_embd'):
            return config.n_embd
        else:
            raise ValueError(f"æ— æ³•ä»æ¨¡å‹é…ç½®ä¸­è·å–embed_dimã€‚æ¨¡å‹è·¯å¾„: {llm_model_path}")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}ã€‚æ¨¡å‹è·¯å¾„: {llm_model_path}")

class RevIN(nn.Module):
    def __init__(self, eps: float = 1e-4):
        super().__init__()
        self.eps = eps

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        if x.dtype != torch.float32:
            pass
        orig_dtype = x.dtype
        x_fp32 = x.float()
        
        mu = x_fp32.mean(dim=1, keepdim=True)
        sigma = x_fp32.std(dim=1, keepdim=True).clamp_min(self.eps)
        
        x_norm = (x_fp32 - mu) / sigma
        x_norm = x_norm.to(dtype=orig_dtype)
        stats = torch.stack((mu.squeeze(1), sigma.squeeze(1)), dim=-1).to(dtype=orig_dtype)
        return x_norm, stats


class FixedSinePositionalEncoding(nn.Module):
    """æ ‡å‡†å›ºå®šæ­£å¼¦ä½ç½®ç¼–ç ã€‚"""
    def __init__(self, dim: int, scale: float = 10000.0):
        super().__init__()
        self.dim = dim
        self.scale = scale

    def forward(self, length: int, device: torch.device, dtype: torch.dtype) -> Tensor:
        position = torch.arange(length, device=device, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, self.dim, 2, device=device, dtype=torch.float32)
            * (-math.log(self.scale) / self.dim)
        )
        pe = torch.zeros(length, self.dim, device=device, dtype=torch.float32)
        pe[:, 0::2] = torch.sin(position * div_term)
        cos_slots = self.dim // 2
        if cos_slots > 0:
            pe[:, 1::2] = torch.cos(position * div_term[:cos_slots])
        return pe.to(dtype=dtype)


class InputPreprocessor(nn.Module):
    """æ¨¡å— Iï¼šè¾“å…¥é¢„å¤„ç† + å»é‡çº² + æ—¶é—´ç¼–ç ã€‚"""
    def __init__(self, config: CROMEConfig):
        super().__init__()
        self.config = config
        self.revin = RevIN(config.epsilon)

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        b, l, c = x.shape
        x_norm, stats = self.revin(x)
        return x_norm,stats


class PatchEmbedding(nn.Module):
    def __init__(self, config: CROMEConfig, in_dim: int):
        super().__init__()
        self.config = config
        self.patch_len = config.patch_len
        self.patch_stride = config.patch_stride
        self.project = nn.Linear(self.patch_len * in_dim, config.patch_embedding_dim)

    def forward(self, x: Tensor) -> Tensor:
        b, _, c = x.shape
        patches = x.unfold(dimension=1, size=self.patch_len, step=self.patch_stride)
        n = patches.shape[1]
        patches = patches.contiguous().view(b, n, self.patch_len * c)
        return self.project(patches)


class PatchTSTEncoder(nn.Module):
    """æ¨¡å— IIï¼šåŸºäº RoPE çš„ PatchTST ç¼–ç å™¨ã€‚"""
    def __init__(self, config: CROMEConfig, input_dim: int):
        super().__init__()
        self.config = config
        
        # 1. Patch Embedding (ä¿æŒä¸å˜)
        self.embedding = PatchEmbedding(config, input_dim)
        
        # 2. RoPE ç”Ÿæˆå™¨ (æ›¿ä»£åŸæœ‰çš„ FixedSinePositionalEncoding)
        # head_dim = embedding_dim / num_heads
        head_dim = config.patch_embedding_dim // config.patch_num_heads
        self.rotary_emb = RotaryEmbedding(dim=head_dim)

        # 3. Transformer Blocks (æ›¿ä»£åŸæœ‰çš„ nn.TransformerEncoder)
        self.blocks = nn.ModuleList([
            RoPETransformerBlock(
                dim=config.patch_embedding_dim,
                num_heads=config.patch_num_heads,
                mlp_ratio=4.0,
                dropout=0.1 # å¦‚æœ config æœ‰ dropout å‚æ•°å¯æ›¿æ¢
            )
            for _ in range(config.patch_num_layers)
        ])
        
        # 4. Final Norm (Pre-Norm ç»“æ„é€šå¸¸éœ€è¦åœ¨æœ€ååŠ ä¸€å±‚ Norm)
        self.norm = nn.LayerNorm(config.patch_embedding_dim)

        if config.freeze_patch_encoder:
            self._freeze()

    def _freeze(self) -> None:
        for p in self.parameters():
            p.requires_grad_(False)

    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:
        # x: [Batch, Seq_Len, Channels]
        
        # 1. Patchify & Project -> [Batch, Num_Patches, Patch_Dim]
        emb = self.embedding(x) 
        
        # 2. Forward through Blocks with RoPE
        # æ³¨æ„ï¼šä¸å†éœ€è¦ self.pos_encoding(emb) çš„åŠ æ³•æ“ä½œ
        
        x_out = emb
        for block in self.blocks:
            x_out = block(x_out, self.rotary_emb)
        
        x_out = self.norm(x_out)

        if self.config.freeze_patch_encoder:
            # è¿”å›åŸå§‹ emb å’Œç¼–ç åçš„ç‰¹å¾
            return emb, x_out.detach()
        else:
            return emb, x_out

class QFormerLayer(nn.Module):
    """
    Q-Former çš„å•å±‚ Blockã€‚
    æ‰§è¡Œé¡ºåºï¼šSelf-Attn (Query) -> Cross-Attn (Text) -> Cross-Attn (Time Series) -> FFN
    """
    def __init__(self, config: CROMEConfig):
        super().__init__()
        dim = config.patch_embedding_dim
        num_heads = config.patch_num_heads
        dropout = getattr(config, "dropout", 0.1)
        
        # 1. Self-Attention: Query Tokens å†…éƒ¨äº¤äº’
        self.self_attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(dim)
        
        # 2. Text Cross-Attention: æ–‡æœ¬å¼•å¯¼ (Instruction)
        self.text_attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)
        self.norm2 = nn.LayerNorm(dim)
        # æ–‡æœ¬æŠ•å½±å±‚ï¼šå°† LLM ç»´åº¦çš„æ–‡æœ¬æ˜ å°„åˆ° Q-Former ç»´åº¦
        self.text_proj = nn.Linear(config.llm_embed_dim, dim) 
        
        # 3. Time-Series Cross-Attention: æ—¶åºç‰¹å¾æå–
        self.ts_attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)
        self.norm3 = nn.LayerNorm(dim)
        
        # 4. Feed Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Linear(dim * 4, dim)
        )
        self.norm4 = nn.LayerNorm(dim)
        
        self.dropout = nn.Dropout(dropout)
        
        # ä¿å­˜æœ€åä¸€æ¬¡çš„ Attention æƒé‡ç”¨äºå¯è§†åŒ–
        self.last_text_attn_weights = None
        self.last_ts_attn_weights = None

    def forward(self, queries, text_embeds, ts_embeds):
        """
        queries: [Batch, Num_Queries, Dim]
        text_embeds: [Batch, Text_Len, LLM_Dim]
        ts_embeds: [Batch, Num_Patches, Dim]
        """
        # 1. Self-Attention (Q=K=V=Queries)
        # Query ä¹‹é—´ç›¸äº’äº¤æµï¼Œæ•´åˆä¸Šä¸€å±‚æå–åˆ°çš„ä¿¡æ¯
        q_out, _ = self.self_attn(queries, queries, queries)
        queries = self.norm1(queries + self.dropout(q_out))
        
        # 2. Text Interaction (æ–‡æœ¬å¼•å¯¼)
        if text_embeds is not None:
            text_embeds_input = text_embeds.to(self.text_proj.weight.dtype)
            
            # æŠ•å½±æ–‡æœ¬ç‰¹å¾åˆ°å½“å‰ç»´åº¦
            text_kv = self.text_proj(text_embeds_input).to(queries.dtype)
            
            # Cross-Attention: Query å…³æ³¨ Text
            text_out, text_attn = self.text_attn(queries, text_kv, text_kv)
            queries = self.norm2(queries + self.dropout(text_out))
            
            self.last_text_attn_weights = text_attn.detach().cpu()
        
        # 3. TS Extraction (æ—¶åºæå–)
        ts_out, ts_attn = self.ts_attn(queries, ts_embeds, ts_embeds)
        queries = self.norm3(queries + self.dropout(ts_out))
        
        self.last_ts_attn_weights = ts_attn.detach().cpu()
        
        # 4. FFN
        queries = self.norm4(queries + self.dropout(self.ffn(queries)))
        
        return queries


class QFormer(nn.Module):
    """
    å¤šå±‚è¿­ä»£å¼ Q-Former (Iterative Q-Former)ã€‚
    é€šè¿‡å¤šå±‚äº¤äº’ï¼Œå®ç°æ·±åº¦çš„æ–‡æœ¬å¼•å¯¼å’Œç‰¹å¾ç²¾ç‚¼ã€‚
    """
    def __init__(self, config: CROMEConfig):
        super().__init__()
        self.config = config
        
        # 1. å¯å­¦ä¹ çš„ Query Tokens (Base Queries)
        # è¿™äº› Token æ˜¯ç‰¹å¾æå–çš„â€œç§å­â€
        self.query_tokens = nn.Parameter(
            torch.randn(config.query_tokens, config.patch_embedding_dim)
        )
        # ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œè¿™é€šå¸¸æ¯”å…¨0åˆå§‹åŒ–æ›´å®¹æ˜“è®­ç»ƒ
        nn.init.normal_(self.query_tokens, std=0.02)
        
        # 2. å †å å¤šå±‚ QFormerLayer
        # å»ºè®®å±‚æ•°ï¼š4 åˆ° 6 å±‚ã€‚å¦‚æœ config ä¸­æ²¡æœ‰å®šä¹‰ï¼Œé»˜è®¤ä½¿ç”¨ 4 å±‚ã€‚
        # ä½ å¯ä»¥åœ¨ CROMEConfig ä¸­æ·»åŠ  qformer_layers å±æ€§æ¥æ§åˆ¶
        num_layers = getattr(config, "qformer_layers", 4) 
        
        self.layers = nn.ModuleList([
            QFormerLayer(config) for _ in range(num_layers)
        ])
        
        # æš´éœ²æœ€åä¸€å±‚çš„ attention map ä¾›å¤–éƒ¨è°ƒç”¨ (ä¾‹å¦‚ plot.py)
        self.last_text_attn_weights = None

    def forward(self, patch_tokens: Tensor, instruction_embeds: Optional[Tensor] = None) -> Tensor:
        """
        patch_tokens: [Batch, Num_Patches, Dim] (æ¥è‡ª Encoder)
        instruction_embeds: [Batch, Text_Len, LLM_Dim] (æ¥è‡ª LLM)
        """
        b = patch_tokens.size(0)
        
        # 1. æ‰©å±• Query Tokens: [1, N_q, D] -> [B, N_q, D]
        queries = self.query_tokens.unsqueeze(0).expand(b, -1, -1)
        
        # 2. è¿­ä»£å¼æ›´æ–° Query
        # æ¯ä¸€å±‚ Query éƒ½ä¼šå˜å¾—æ›´åŠ â€œèªæ˜â€ï¼Œå› ä¸ºå®ƒåå¤çœ‹äº†æ–‡æœ¬å’Œæ•°æ®
        for layer in self.layers:
            queries = layer(queries, instruction_embeds, patch_tokens)
            
        # 3. è®°å½•æœ€åä¸€å±‚çš„ Attention Map (ä¸ºäº†å…¼å®¹æ—§çš„å¯è§†åŒ–ä»£ç )
        self.last_text_attn_weights = self.layers[-1].last_text_attn_weights
            
        return queries

class DetailProjection(nn.Module):
    def __init__(self, config: CROMEConfig, input_dim: int = None):
        super().__init__()
        
        dim = input_dim if input_dim is not None else config.patch_embedding_dim
        target_dim = config.patch_embedding_dim 
        
        hidden_dim = target_dim * 4  
        drop_rate = config.proj_dropout

        self.proj = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(drop_rate),
            
            nn.Linear(hidden_dim, target_dim)
        )
        
        self._init_weights()

    def _init_weights(self):
        for m in self.proj.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, patch_tokens: Tensor) -> Tensor:
        return self.proj(patch_tokens)

class RobustFiLMGenerator(nn.Module):
    """
    Robust Log-Space FiLM Generator.
    """
    def __init__(self, config: CROMEConfig):
        super().__init__()
        # input_dim = 3 (log_mu, log_sigma, sign_mu)
        input_dim = 3
        
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, config.adapter_hidden_dim),
            nn.LayerNorm(config.adapter_hidden_dim),
            nn.SiLU(),
            nn.Linear(config.adapter_hidden_dim, config.adapter_hidden_dim * 2) # gamma, beta
        )
        
        nn.init.zeros_(self.mlp[-1].weight)
        nn.init.zeros_(self.mlp[-1].bias)

    def forward(self, stats: Tensor) -> Tuple[Tensor, Tensor]:
        if stats.dim() == 2:
            mu = stats[..., 0:1]
            sigma = stats[..., 1:2]
        else:
            mu = stats[..., 0]
            sigma = stats[..., 1]
            
        log_mu = torch.log1p(mu.abs())
        log_sigma = torch.log1p(sigma)
        sign_mu = torch.sign(mu)
        
        features = torch.cat([log_mu, log_sigma, sign_mu], dim=-1)
        out = self.mlp(features)
        
        if out.dim() == 2:
            out = out.unsqueeze(1) 
            
        gamma, beta = out.chunk(2, dim=-1)
        return gamma, beta


class CROMEAdapterBlock(nn.Module):
    def __init__(self, dim: int, hidden_dim: int):
        super().__init__()
        self.down = nn.Linear(dim, hidden_dim)
        self.gate = nn.Linear(dim, hidden_dim)
        self.up = nn.Linear(hidden_dim, dim)

    def forward(self, x: Tensor, gamma: Optional[Tensor] = None, beta: Optional[Tensor] = None) -> Tensor:
        z = F.silu(self.down(x)) * self.gate(x)
        if gamma is not None and beta is not None:
            z = z * (1 + gamma) + beta
        return x + self.up(z)


# --- [ä¿®æ”¹] 3. é€‚é…å™¨ (ä¸¢å¼ƒ Detail Tokens è¾“å…¥) ---
class CROMEAdapter(nn.Module):
    def __init__(self, config: CROMEConfig):
        super().__init__()
        # åªéœ€è¦ä¸€ä¸ª Query Adapter
        # ç§»é™¤äº†åŸæœ‰çš„ patch_adapter
        self.query_adapter = CROMEAdapterBlock(
            config.patch_embedding_dim, config.adapter_hidden_dim
        )

    def forward(
        self, 
        query_tokens: Tensor, 
        # patch_tokens: Tensor,  <-- ç§»é™¤æ­¤å‚æ•°
        # sep_embed: Optional[Tensor] = None, <-- ç§»é™¤æ­¤å‚æ•°
        gamma: Optional[Tensor] = None, 
        beta: Optional[Tensor] = None
    ) -> Tensor:
        
        # [å…³é”®ä¿®æ”¹] å°† FiLM ç»Ÿè®¡é‡ (Gamma/Beta) æ³¨å…¥åˆ° Query Tokens ä¸­
        # è¿™ç¡®ä¿äº†å³ä½¿ä¸¢å¼ƒäº† Detail Tokensï¼ŒLLM ä¾ç„¶èƒ½æ„ŸçŸ¥åˆ°åºåˆ—çš„å¹…åº¦ä¿¡æ¯
        return self.query_adapter(query_tokens, gamma=gamma, beta=beta)


class InstructionTokenizer:
    def __init__(self, config: CROMEConfig):
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.llm_model_path, use_fast=False
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"

    def __call__(self, texts: Sequence[str], device: torch.device) -> Dict[str, Tensor]:
        encoded = self.tokenizer(
            list(texts),
            return_tensors="pt",
            padding=True,
            truncation=True,
        )
        return {k: v.to(device) for k, v in encoded.items()}


class FrozenLLM(nn.Module):
    def __init__(self, config: CROMEConfig):
        super().__init__()
        dtype = _resolve_dtype(config.llm_dtype)
        self.model = AutoModelForCausalLM.from_pretrained(
            config.llm_model_path,
            torch_dtype=dtype,
            device_map=config.llm_device_map,
            attn_implementation="flash_attention_2"
        )
        self.model.eval()
        for p in self.model.parameters():
            p.requires_grad_(False)

    @property
    def embed_layer(self) -> nn.Embedding:
        return self.model.get_input_embeddings()

    def embed(self, input_ids: Tensor) -> Tensor:
        embed_layer = self.embed_layer
        target_device = input_ids.device
        weight_device = embed_layer.weight.device
        if target_device != weight_device:
            input_ids = input_ids.to(weight_device)
        embeds = embed_layer(input_ids)
        if embeds.device != target_device:
            embeds = embeds.to(target_device)
        return embeds

    def forward(self, inputs_embeds: Tensor, attention_mask: Tensor, **kwargs):
        return self.model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            **kwargs,
        )
    
class SeriesDecomp(nn.Module):
    """
    åºåˆ—åˆ†è§£æ¨¡å—ï¼šå°†åºåˆ—åˆ†è§£ä¸º è¶‹åŠ¿é¡¹(Trend) å’Œ æ®‹å·®é¡¹(Residual)ã€‚
    X_residual = X_input - MovingAvg(X_input)
    """
    def __init__(self, kernel_size):
        super().__init__()
        self.kernel_size = kernel_size
        # ä½¿ç”¨å¹³å‡æ± åŒ–å®ç°ç§»åŠ¨å¹³å‡
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=0)

    def forward(self, x):
        # x: [Batch, Seq_Len, Channels]
        
        # Padding ä»¥ä¿æŒåºåˆ—é•¿åº¦ä¸å˜
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x_pad = torch.cat([front, x, end], dim=1)
        
        # è®¡ç®— Trend
        # [B, L, C] -> [B, C, L] -> AvgPool -> [B, C, L] -> [B, L, C]
        x_trend = self.avg(x_pad.permute(0, 2, 1)).permute(0, 2, 1)
        
        # è®¡ç®— Residual (è¿™å°±åŒ…å«äº† Noise å’Œ Anomaly)
        x_resid = x - x_trend
        
        return x_resid, x_trend

# ==========================================
# 1. ä¿®æ”¹ CNNDetailEncoder: çº¯å¾®è§‚ ResNet-1D
# ==========================================
class CNNDetailEncoder(nn.Module):
    """
    çº¯å¾®è§‚ç»†èŠ‚ç¼–ç å™¨ (Pure Micro Encoder)ã€‚
    ç‰¹ç‚¹ï¼š
    1. æ— æ± åŒ– (No Pooling)ï¼šç»å¯¹ä¸ä¸¢å¤±é«˜é¢‘ç»†èŠ‚ã€‚
    2. æ— è†¨èƒ€ (No Dilation)ï¼šåªå…³æ³¨å±€éƒ¨ (Kernel=3)ï¼Œä¸è¶Šç•Œå»ç®¡å®è§‚ã€‚
    3. ResNetç»“æ„ï¼šæ·±å±‚ç‰¹å¾æå–ï¼Œè®­ç»ƒç¨³å®šã€‚
    """
    def __init__(self, input_channels: int, patch_embedding_dim: int, dropout: float = 0.1):
        super().__init__()
        
        # éšè—å±‚ç»´åº¦
        hidden_dim = 64 
        
        # ç¬¬ä¸€å±‚æŠ•å½±: å°†è¾“å…¥ (Resid+Diff) æ˜ å°„åˆ°éšè—ç©ºé—´
        self.first_conv = nn.Conv1d(input_channels, hidden_dim, kernel_size=1)
        
        # å®šä¹‰ 3 ä¸ªæ ‡å‡†çš„æ®‹å·®å—
        # åšæŒ Kernel=3, Dilation=1 (ä¸è†¨èƒ€), Padding=1 (ä¿æŒé•¿åº¦)
        # è¿™ç§ç»“æ„åªå…³æ³¨ "t" æ—¶åˆ»åŠå…¶å·¦å³é‚»å±…ï¼Œéå¸¸ç¬¦åˆ "Micro" çš„å®šä¹‰
        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm1d(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False),
                nn.BatchNorm1d(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout)
            ) for _ in range(3) # å †å  3 å±‚ï¼Œè¶³ä»¥æå–å¤æ‚çš„å¾®è§‚ç‰¹å¾
        ])
        
        # æœ€ç»ˆæŠ•å½±åˆ° PatchTST çš„ç»´åº¦ï¼Œä»¥ä¾¿ Q-Former å¤„ç†
        self.final_proj = nn.Conv1d(hidden_dim, patch_embedding_dim, kernel_size=1)

    def forward(self, x):
        # x input shape: [Batch, Channels, Seq_Len]
        # Channels = 2 (Resid + Diff)
        
        x = self.first_conv(x)
        
        for layer in self.layers:
            residual = x
            x = layer(x)
            x = x + residual # ResNet é£æ ¼è¿æ¥
            
        x = self.final_proj(x)
        
        # è¾“å‡ºå½¢çŠ¶: [Batch, Dim, Seq_Len]
        # è½¬ç½®ä¸º [Batch, Seq_Len, Dim] ç»™ Q-Former
        return x.transpose(1, 2)


# ==========================================
# 2. ä¿®æ”¹ CROMETSModel: åˆå§‹åŒ–ä¸åŒæµé€»è¾‘
# ==========================================
class CROMETSModel(nn.Module):
    def __init__(self, config: CROMEConfig):
        super().__init__()
        self.config = config
        
        # 1. é¢„å¤„ç†
        self.preprocessor = InputPreprocessor(config)
        
        # 2. Macro æµç¼–ç å™¨
        pre_input_dim = (
            config.input_channels if config.fuse_mode == "add" else config.input_channels * 2
        )
        self.shape_encoder = PatchTSTEncoder(config, pre_input_dim)
        
        # 3. Micro æµç¼–ç å™¨ (çº¯å¾®è§‚ ResNet)
        self.decomp = SeriesDecomp(kernel_size=65) # ä¿æŒ Kernel=65
        self.detail_encoder = CNNDetailEncoder(
            input_channels=2, # Resid + Diff
            patch_embedding_dim=config.patch_embedding_dim,
            dropout=config.proj_dropout
        )
        
        # ==========================================
        # [æ–°å¢] 4. æŒ‡ä»¤è’¸é¦å™¨
        # ==========================================
        self.instr_refiner = InstructionRefiner(config)
        
        # 5. Q-Former, FiLM, Adapter
        self.qformer = QFormer(config)
        self.film_generator = RobustFiLMGenerator(config)
        self.adapter = CROMEAdapter(config)
        
        # 6. èåˆé—¨æ§
        self.fusion_gate = FusionGate(config)
        
        # 7. LLM æŠ•å½±
        self.llm_proj = nn.Sequential(
            nn.Linear(config.patch_embedding_dim, config.llm_embed_dim),
            nn.GELU(),
            nn.Dropout(config.proj_dropout),
            nn.Linear(config.llm_embed_dim, config.llm_embed_dim)
        )

    def _process_single_channel(
        self,
        channel_data: Tensor,
        instruction_embeds: Optional[Tensor] = None,
        sep_embed: Optional[Tensor] = None,
    ) -> Tensor:
        # =========================================================
        # [æ­¥éª¤ 0] æŒ‡ä»¤è’¸é¦ (Instruction Refinement)
        # =========================================================
        # å°†åŸå§‹çš„å«æŒ‡ä»£æŒ‡ä»¤ (Raw) è½¬åŒ–ä¸ºçº¯ä»»åŠ¡æŒ‡ä»¤ (Refined)
        # task_embeds: [B, Num_Task_Queries, LLM_Dim]
        # è¿™é‡Œçš„ task_embeds å°†æ›¿ä»£ raw instruction_embeds ä¼ ç»™ä¸‹æ¸¸
        task_embeds = self.instr_refiner(instruction_embeds)
        
        # å¦‚æœè’¸é¦å™¨è¿”å› None (å³æ²¡æœ‰è¾“å…¥æŒ‡ä»¤)ï¼Œåˆ™æ²¿ç”¨ Noneï¼Œ
        # ä¸‹æ¸¸çš„ Gate ä¼šå¤„ç† None (è¿”å› 0.5)ï¼ŒQFormer ä¹Ÿä¼šå¤„ç† None (ä¸è¿›è¡Œ Text Attention)
        
        # 1. é¢„å¤„ç†ä¸åˆ†è§£
        x, stats = self.preprocessor(channel_data)
        gamma, beta = self.film_generator(stats)
        
        # Kernel=65 åˆ†è§£
        x_resid, x_trend = self.decomp(x)
        
        # 2. Macro æµï¼šå…œåº•çœ‹ Raw
        _, deep_feats = self.shape_encoder(x)
        
        # 3. Micro æµï¼šResid + Diff
        x_t = x.permute(0, 2, 1)        # [B, 1, L]
        x_resid_t = x_resid.permute(0, 2, 1) # [B, 1, L]
        x_diff_t = torch.diff(x_t, dim=2, prepend=x_t[:, :, :1])
        
        micro_input = torch.cat([x_resid_t, x_diff_t], dim=1) # [B, 2, L]
        detail_feats = self.detail_encoder(micro_input)
        
        # =========================================================
        # 4. ç‹¬ç«‹æŸ¥è¯¢ (ä½¿ç”¨ task_embeds æ›¿ä»£ instruction_embeds)
        # =========================================================
        
        # (A) æŸ¥è¯¢å®è§‚è¶‹åŠ¿
        # Q-Former å†…éƒ¨ Cross-Attn ç°åœ¨çœ‹åˆ°çš„æ˜¯çº¯ç²¹çš„ä»»åŠ¡è¯­ä¹‰
        q_macro = self.qformer(deep_feats, task_embeds)
        
        # (B) æŸ¥è¯¢å¾®è§‚ç»†èŠ‚
        q_micro = self.qformer(detail_feats, task_embeds)
        
        # 5. åŠ¨æ€èåˆ (ä½¿ç”¨ task_embeds)
        # Gate ä¼šå¯¹ task_embeds è¿›è¡Œ Poolingï¼Œç„¶åå†³å®šæƒé‡
        # ç”±äº task_embeds æ˜¯â€œæçº¯â€è¿‡çš„ï¼ŒGate åˆ¤åˆ«ä¼šæ›´å‡†
        gate = self.fusion_gate(task_embeds) 
        
        q_fused = gate * q_macro + (1 - gate) * q_micro
        
        # 6. Adapter & Projection
        ts_tokens = self.adapter(q_fused, gamma=gamma, beta=beta)
        ts_tokens = self.llm_proj(ts_tokens)
        
        return ts_tokens
    # def _process_single_channel(
    #     self,
    #     channel_data: Tensor,
    #     instruction_embeds: Optional[Tensor] = None,
    #     sep_embed: Optional[Tensor] = None, # 
    # ) -> Tensor:
    #     x, stats = self.preprocessor(channel_data)
    #     gamma, beta = self.film_generator(stats)
        
    #     # æ¥æ”¶åŒæµç‰¹å¾
    #     raw_embeds, deep_feats = self.shape_encoder(x)
        
    #     # 1. Q-Former (è¯­ä¹‰)
    #     query_tokens = self.qformer(deep_feats, instruction_embeds)
        
    #     # 2. Detail Projector (ç»†èŠ‚)
    #     detail_tokens = self.detail_proj(raw_embeds) 
        
    #     # 3. èåˆ (ä¼ å…¥ sep_embed)
    #     ts_tokens = self.adapter(
    #         query_tokens, detail_tokens, 
    #         sep_embed=sep_embed, 
    #         gamma=gamma, beta=beta
    #     )
    #     ts_tokens = self.llm_proj(ts_tokens)
        
    #     return ts_tokens

    # def forward(
    #     self,
    #     raw_series: Tensor,
    #     text_prefix: Tensor,
    #     text_suffix: Tensor,
    #     instruction_embeds: Optional[Tensor] = None,
    # ) -> Dict[str, Tensor]:
    #     target_dtype = text_prefix.dtype
    #     x, stats = self.preprocessor(raw_series)
    #     gamma, beta = self.film_generator(stats)
        
    #     raw_embeds, deep_feats = self.shape_encoder(x)
        
    #     query_tokens = self.qformer(deep_feats, instruction_embeds)
    #     detail_tokens = self.detail_proj(raw_embeds)
        
    #     # é€šç”¨ forward æš‚æ—¶ä¸å¤„ç† sep_embedï¼Œæˆ–è€…ä¹Ÿå¯ä»¥åŠ ä¸Š
    #     ts_tokens = self.adapter(query_tokens, detail_tokens, gamma=gamma, beta=beta)
    #     ts_tokens = self.llm_proj(ts_tokens)
        
    #     if ts_tokens.dtype != target_dtype:
    #         ts_tokens = ts_tokens.to(dtype=target_dtype)
            
    #     assembled = torch.cat(
    #         [text_prefix, ts_tokens, text_suffix],
    #         dim=1,
    #     )
    #     return {
    #         "ts_tokens": ts_tokens,
    #         "assembled": assembled,
    #     }


class StatBypassCROMETS1(nn.Module):
    def __init__(self, config: CROMEConfig):
        super().__init__()
        self.config = config
        self.ts_model = CROMETSModel(config)
        self.llm = FrozenLLM(config)
        self.tokenizer = InstructionTokenizer(config)
        
        self.sep_token = nn.Parameter(
            torch.randn(1, config.llm_embed_dim) * 0.02
        )
        
        #  å®šä¹‰æ¨¡æ€ç‰¹æ®Šæ ‡è®° (éšæœºåˆå§‹åŒ–)
        # ä½¿ç”¨ nn.Parameter ç¡®ä¿åœ¨ SFT/LoRA æ—¶èƒ½è¢«ä¼˜åŒ–å™¨æ•è·
        self.ts_start_token = nn.Parameter(torch.randn(1, 1, config.llm_embed_dim) * 0.02)
        self.ts_end_token   = nn.Parameter(torch.randn(1, 1, config.llm_embed_dim) * 0.02)
        # self.feat_sep_token = nn.Parameter(torch.randn(1, 1, config.llm_embed_dim) * 0.02)
        # self.feat_sep_token = nn.Parameter(torch.randn(1, 1, config.patch_embedding_dim) * 0.02)

    def _prepare_text(
        self,
        text_input: Union[Tensor, Sequence[str]],
        device: torch.device,
    ) -> Tuple[Tensor, Tensor]:
        if isinstance(text_input, Tensor):
            mask = torch.ones(
                text_input.size(0),
                text_input.size(1),
                dtype=torch.long,
                device=device,
            )
            return text_input, mask
        encoded = self.tokenizer(text_input, device)
        embeds = self.llm.embed(encoded["input_ids"])
        return embeds, encoded["attention_mask"]
    def prepare_multimodal_embeds(
        self,
        input_texts: Sequence[str],
        timeseries_lists: Sequence[Sequence[Tensor]],
        output_texts: Optional[Sequence[str]] = None,
        # æ–°å¢ Mask å‚æ•°ç”¨äº Ablation
        mask_query: bool = False,
        mask_detail: bool = False,
        mask_text_stats: bool = False
    ) -> Dict[str, Any]:
        """
        æ ¸å¿ƒé€»è¾‘å°è£…ï¼šç»Ÿä¸€æ„å»ºè®­ç»ƒå’Œæ¨ç†ç”¨çš„ Multimodal Embeddingsã€‚
        æ‹¼æ¥é¡ºåºï¼š[Start] -> [Stats] -> [TS Features] -> [End] 
        """
        device = next(self.parameters()).device
        batch_size = len(input_texts)
        
        assembled_embeds_list = []
        attention_masks_list = []
        prefix_mask_lengths = []
        suffix_mask_lengths = []
        
        target_dtype = next(self.llm.parameters()).dtype
        
        for i in range(batch_size):
            input_text = input_texts[i]
            timeseries_list = list(timeseries_lists[i]) 
            output_text = output_texts[i] if output_texts is not None else None
            
            ts_marker = "<ts><ts/>"
            text_parts = input_text.split(ts_marker)
            
            # 1. æå–å…¨å±€æŒ‡ä»¤ (Text-Guided)
            full_instruction_text = " ".join([p.strip() for p in text_parts if p.strip()])
            current_instruction_embeds = None
            drop_text = self.training and (torch.rand(1).item() < 0.15)
            
            if full_instruction_text and not drop_text:
                instr_encoded = self.tokenizer([full_instruction_text], device)
                input_ids = instr_encoded["input_ids"]
                if input_ids.shape[1] > 2048:
                     input_ids = input_ids[:, :2048]
                current_instruction_embeds = self.llm.embed(input_ids)
            
            num_markers = len(text_parts) - 1
            num_timeseries = len(timeseries_list)
            
            if num_timeseries < num_markers:
                for _ in range(num_markers - num_timeseries):
                    timeseries_list.append(
                        torch.zeros(self.config.input_channels, device=device)
                    )
            elif num_timeseries > num_markers:
                timeseries_list = timeseries_list[:num_markers]
            
            segment_embeds = []
            segment_masks = []
            
            # 2. Prefix Text
            if text_parts[0]:
                prefix_encoded = self.tokenizer([text_parts[0]], device)
                prefix_embed = self.llm.embed(prefix_encoded["input_ids"])
                prefix_mask = prefix_encoded["attention_mask"]
                segment_embeds.append(prefix_embed[0])
                segment_masks.append(prefix_mask[0])
                prefix_length = prefix_mask[0].sum().item()
            else:
                prefix_length = 0
            
            prefix_mask_lengths.append(prefix_length)
            
            # 3. Time Series Loop
            for ts_idx, ts_tensor in enumerate(timeseries_list):
                ts_tensor = ts_tensor.to(device)
                
                if ts_tensor.numel() > 0:
                    ts_mean = ts_tensor.mean().item()
                    ts_std = ts_tensor.std().item()
                else:
                    ts_mean = 0.0
                    ts_std = 1.0
                
                # Stats Dropout / Mask Logic
                if mask_text_stats:
                    stats_str = ""
                elif self.training and torch.rand(1).item() < 0.5:
                    stats_str = ""
                else:
                    stats_str = f" [Scale: {ts_std:.2f}, Offset: {ts_mean:.2f}] "
                
                # è·å–æ—¶åºç‰¹å¾ (Output: [Query, Sep, Detail])
                ts_batch = ts_tensor.unsqueeze(0)
                ts_tokens = self.ts_model._process_single_channel(
                    ts_batch, 
                    instruction_embeds=current_instruction_embeds
                )
                
                if mask_query:
                    ts_tokens.fill_(0.0)

                if ts_tokens.dtype != target_dtype:
                    ts_tokens = ts_tokens.to(dtype=target_dtype)
                ts_embed = ts_tokens[0]
                
                # (A) Start Token
                segment_embeds.append(self.ts_start_token[0]) 
                segment_masks.append(torch.ones(1, device=device, dtype=torch.long))
                
                # (B) Stats Text (å‰ç½®!)
                if stats_str:
                    stats_encoded = self.tokenizer([stats_str], device)
                    stats_embed = self.llm.embed(stats_encoded["input_ids"])
                    stats_mask = stats_encoded["attention_mask"]
                    segment_embeds.append(stats_embed[0])
                    segment_masks.append(stats_mask[0])
                
                # (C) TS Features
                segment_embeds.append(ts_embed)
                segment_masks.append(torch.ones(ts_embed.shape[0], device=device, dtype=torch.long))
                
                # (D) End Token
                segment_embeds.append(self.ts_end_token[0])
                segment_masks.append(torch.ones(1, device=device, dtype=torch.long))
                
                # (E) Multi-TS Separator (å¯é€‰)
                if ts_idx < len(timeseries_list) - 1:
                    sep_embed = self.sep_token
                    if sep_embed.dtype != target_dtype:
                        sep_embed = sep_embed.to(dtype=target_dtype)
                    segment_embeds.append(sep_embed)
                    segment_masks.append(torch.ones(1, device=device, dtype=torch.long))
                
                # (F) Next Text Part
                text_idx = ts_idx + 1
                if text_idx < len(text_parts) and text_parts[text_idx]:
                    text_encoded = self.tokenizer([text_parts[text_idx]], device)
                    text_embed = self.llm.embed(text_encoded["input_ids"])
                    text_mask = text_encoded["attention_mask"]
                    segment_embeds.append(text_embed[0])
                    segment_masks.append(text_mask[0])
            
            # 4. Suffix Text (Output)
            if output_text:
                suffix_encoded = self.tokenizer([output_text], device)
                suffix_embed = self.llm.embed(suffix_encoded["input_ids"])
                suffix_mask = suffix_encoded["attention_mask"]
                segment_embeds.append(suffix_embed[0])
                segment_masks.append(suffix_mask[0])
                suffix_length = suffix_mask[0].sum().item()
            else:
                suffix_length = 0
            
            suffix_mask_lengths.append(suffix_length)
            
            # Concat for this sample
            full_embed = torch.cat(segment_embeds, dim=0)
            full_mask = torch.cat(segment_masks, dim=0)
            
            assembled_embeds_list.append(full_embed)
            attention_masks_list.append(full_mask)
        
        # 5. Padding Batch
        max_len = max(emb.shape[0] for emb in assembled_embeds_list)
        embed_dim = assembled_embeds_list[0].shape[1]
        
        padded_embeds = torch.zeros(batch_size, max_len, embed_dim, device=device, dtype=assembled_embeds_list[0].dtype)
        padded_masks = torch.zeros(batch_size, max_len, device=device, dtype=torch.long)
        
        for i, (emb, mask) in enumerate(zip(assembled_embeds_list, attention_masks_list)):
            seq_len = emb.shape[0]
            padded_embeds[i, :seq_len] = emb
            padded_masks[i, :seq_len] = mask
            
        padded_embeds = padded_embeds.to(self.llm.model.dtype)
        
        return {
            "inputs_embeds": padded_embeds,
            "attention_mask": padded_masks,
            "prefix_mask_lengths": prefix_mask_lengths,
            "suffix_mask_lengths": suffix_mask_lengths
        }

    def forward_chatts(
        self,
        input_texts: Sequence[str],
        timeseries_lists: Sequence[Sequence[Tensor]],
        output_texts: Sequence[str],
        llm_kwargs: Optional[Dict] = None,
    ) -> Dict[str, Any]:
        """è®­ç»ƒæ—¶è°ƒç”¨ï¼Œè®¡ç®— Loss"""
        llm_kwargs = llm_kwargs or {}
        
        prepared = self.prepare_multimodal_embeds(input_texts, timeseries_lists, output_texts)
        
        outputs = self.llm(
            inputs_embeds=prepared["inputs_embeds"],
            attention_mask=prepared["attention_mask"],
            **llm_kwargs,
        )
        
        return {
            "llm_outputs": outputs,
            **prepared
        }

    @torch.no_grad()
    def generate(
        self,
        input_texts: Union[str, Sequence[str]],
        timeseries_lists: Union[Sequence[Tensor], Sequence[Sequence[Tensor]]],
        mask_query: bool = False,
        mask_detail: bool = False,
        mask_text_stats: bool = False,
        **gen_kwargs
    ):
        """
        æ¨ç†æ—¶è°ƒç”¨ï¼Œæ”¯æŒ Ablation Maskingã€‚
        """
        if isinstance(input_texts, str):
            input_texts = [input_texts]
        if len(timeseries_lists) > 0 and isinstance(timeseries_lists[0], Tensor):
            timeseries_lists = [timeseries_lists]
            
        prepared = self.prepare_multimodal_embeds(
            input_texts, 
            timeseries_lists, 
            output_texts=None,
            mask_query=mask_query,
            mask_detail=mask_detail,
            mask_text_stats=mask_text_stats
        )
        
        return self.llm.model.generate(
            inputs_embeds=prepared["inputs_embeds"],
            attention_mask=prepared["attention_mask"],
            **gen_kwargs
        )